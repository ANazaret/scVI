{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonizing starMAP dataset and Saunders's Dropseq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Creating and training the model](#Creating and training the model)\n",
    "2. [Creating and training the original scVI model as a baseline](#Creating and training the original scVI model as a baseline)\n",
    "3. [Imputation](#Imputation)\n",
    "4. [Getting a common meaningful representation](#Getting a common meaningful representation)\n",
    "5. [Classifying starMAP cells in different cell types](#Classifying starMAP cells in different cell types)\n",
    "6. [Imputation of non-observed genes for starMAP](#Imputation of non-observed genes for starMAP)\n",
    "\n",
    "## Imputation\n",
    "We train our model and the baseline without observing some starMAP genes . We then try to reconstruct the unobserved values for each cell, an compare them with the real ones. \n",
    "For the baseline we use a k-NN approach, and for our model we directly output the expected counts.\n",
    "We also compute, for each of those unobserved genes, the absolute and relative errors.\n",
    "## Getting a common meaningful representation\n",
    "Here, we'd like two things. First, the two datasets should mix pretty well (if the common representation captures biologically relevant information). This is measured by the Entropy of Batch Mixing (maximum possible value: 0.68, minimum possible value: 0.00, value for our method: 0.50, value for baseline: 0.10).\n",
    "We also plot the latent represnetation of the cells according to their cell types, hoping that cells from different datasets but same cell types lie close to each other in our latent space.\n",
    "To check that our representation is meaningful, we plot the expression of marker genes for sub-cell types for scRNA-seq data. That allows us to see that our representation conserves data structure at the sub cell type level.\n",
    "## Classifying starMAP cells in different cell types\n",
    "Here, we use a k-NN classifier for the baseline, and a SVC classifier on the expected frequencies of the model for our method.\n",
    "## Imputation of non-observed genes for starMAP\n",
    "We start by imputing marker genes for different cell types and show that the expected frequencies are correlated with the expression of other marker genes for the same cell types, to ensure consistency of our model.\n",
    "We then impute a gene supposed to be spatially differentially expressed, and show that the expected counts imputed by our model for this gene are also spatially differentially expressed (last figure of the notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'save_path': 'data/'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('docs/notebooks/scRNA_and_starMAP.config.json') as f:\n",
    "    config = json.load(f)\n",
    "print(config)\n",
    "\n",
    "n_epochs_all = config['n_epochs'] if 'n_epochs' in config else None\n",
    "save_path = config['save_path'] if 'save_path' in config else 'data/'\n",
    "n_samples_tsne = config['n_samples_tsne'] if 'n_samples_tsne' in config else None\n",
    "n_samples_posterior_density = config['n_samples_posterior_density'] if 'n_samples_posterior_density' in config else None\n",
    "train_size = config['train_size'] if 'train_size' in config else None\n",
    "M_sampling = config['M_sampling'] if 'M_sampling' in config else None\n",
    "M_permutation = config['M_permutation'] if 'M_permutation' in config else None\n",
    "rate = config['rate'] if 'rate' in config else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scvi.dataset import DropseqDataset, StarmapDataset\n",
    "from scvi.inference import TrainerFish, UnsupervisedTrainer\n",
    "from scvi.models import VAEF, VAE\n",
    "from scvi.inference.posterior import plot_imputation, proximity_imputation, entropy_batch_mixing\n",
    "from scvi.inference.annotation import compute_accuracy_nn\n",
    "from MNNs import MNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n",
      "Finished preprocessing dataset\n"
     ]
    }
   ],
   "source": [
    "# the genes to impute are selected randomly\n",
    "n_imputed = 2 # number of genes to impute, can be much higher\n",
    "gene_dataset_starmap = StarmapDataset(save_path=save_path)\n",
    "gene_names = gene_dataset_starmap.gene_names\n",
    "genes_to_discard = np.random.choice(np.arange(len(gene_names)), n_imputed, replace=True)\n",
    "genes_discarded = gene_names[genes_to_discard]\n",
    "indexes_to_keep = np.arange(len(gene_names))\n",
    "indexes_to_keep = np.delete(indexes_to_keep, genes_to_discard)\n",
    "# The \"genes_to_discard\" argument is given here so that the order of the genes in DropseqDataset matches\n",
    "# the order in StarmapDataset, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n",
      "Finished preprocessing dataset\n"
     ]
    }
   ],
   "source": [
    "gene_dataset_seq = DropseqDataset(genes_starmap=gene_names[indexes_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 20/20 [22:31<00:00, 67.59s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100 if n_epochs_all is None else n_epochs_all\n",
    "vae = VAEF(gene_dataset_seq.nb_genes, indexes_to_keep, n_layers_decoder=1, n_latent=8,\n",
    "           n_layers_shared=2, n_hidden=256, reconstruction_loss='zinb', dropout_rate=0.3, n_batch=4, model_library=False)\n",
    "trainer = TrainerFish(vae, gene_dataset_seq, gene_dataset_starmap, train_size=0.9, verbose=False, frequency=1, weight_decay=0.30, n_epochs_even=1, n_epochs_kl=1000,\n",
    "                                cl_ratio = 0, n_epochs_cl=150)\n",
    "trainer.train(n_epochs=n_epochs, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving all the information on the dropseq dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_all_latent_and_expected_frequencies() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ee8b70c1c992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_latent_and_expected_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scRNA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlatent_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_frequencies_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"expected_frequencies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgene_dataset_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_all_latent_and_expected_frequencies() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "dic = trainer.get_all_latent_and_expected_frequencies(mode='scRNA')\n",
    "latent_seq, expected_frequencies_seq = dic[\"latent\"], dic[\"expected_frequencies\"]\n",
    "labels_seq = gene_dataset_seq.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving all the information on the starmap dataset\n",
    "It is also possible to save the expected frequencies and the latent space for the starmap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = trainer.get_all_latent_and_expected_frequencies(mode='smFISH')\n",
    "latent_starmap, expected_frequencies_starmap = dic[\"latent\"], dic[\"expected_frequencies\"]\n",
    "labels_starmap = gene_dataset_starmap.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the PCA and MNN baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data for benchmark\n",
    "concatenated_matrix = np.concatenate((gene_dataset_starmap.X[:, vae.indexes_to_keep], gene_dataset_seq.X[:, vae.indexes_to_keep]))\n",
    "non_zero_cells = np.sum(concatenated_matrix, axis=1) > 0\n",
    "concatenated_matrix[non_zero_cells, :] = concatenated_matrix[non_zero_cells] / np.sum(concatenated_matrix[non_zero_cells, :], axis=1)[:, np.newaxis]\n",
    "concatenated_matrix = np.log(1 + 1e4 * concatenated_matrix)\n",
    "pca = PCA(n_components=8)\n",
    "latent_pca = pca.fit_transform(concatenated_matrix)\n",
    "PCA_latent_starmap = latent_pca[:gene_dataset_starmap.X.shape[0], :]\n",
    "PCA_latent_seq = latent_pca[gene_dataset_starmap.X.shape[0]:, :]\n",
    "\n",
    "\n",
    "mnn = MNN()\n",
    "concatenated_matrix = np.concatenate(\n",
    "    (gene_dataset_starmap.X[:, vae.indexes_to_keep], gene_dataset_seq.X[:, vae.indexes_to_keep]))\n",
    "concatenated_matrix = mnn.fit_transform(concatenated_matrix, np.concatenate(\n",
    "    (np.zeros(gene_dataset_starmap.X.shape[0]), np.ones(gene_dataset_seq.X.shape[0]))), [0, 1])\n",
    "mnn = PCA(n_components=8)\n",
    "latent_mnn = mnn.fit_transform(concatenated_matrix)\n",
    "mnn_latent_starmap = latent_mnn[:gene_dataset_starmap.X.shape[0], :]\n",
    "mnn_latent_seq = latent_mnn[gene_dataset_starmap.X.shape[0]:, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating and training the original scVI model as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scVI model is also designed to do batch effect corrections on datasets that contain multiple batches. We are going to use it as a baseline that we compare to this new model that has been created by modifying the scVI model to address specifically this harmonization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scvi():\n",
    "    gene_dataset_starmap = StarmapDataset(save_path=save_path)\n",
    "    gene_names = gene_dataset_starmap.gene_names\n",
    "\n",
    "\n",
    "    gene_dataset_seq_scvi = DropseqDataset(save_path=save_path, genes_starmap=gene_names[indexes_to_keep])\n",
    "    # Uniformying the gene names so that we can concatenate datasets on gene names\n",
    "    gene_dataset_seq_scvi.gene_names = np.array([gene.lower() for gene in gene_dataset_seq_scvi.gene_names])\n",
    "    gene_dataset_starmap.gene_names = np.array([gene.lower() for gene in gene_dataset_starmap.gene_names])\n",
    "\n",
    "    gene_dataset = GeneExpressionDataset.concat_datasets(gene_dataset_starmap, gene_dataset_seq_scvi)\n",
    "\n",
    "    gene_dataset_starmap.gene_names = gene_names\n",
    "    vae = VAE(gene_dataset.nb_genes, n_batch=4,\n",
    "          n_labels=gene_dataset.n_labels, dispersion=\"gene-batch\", reconstruction_loss=\"nb\")\n",
    "\n",
    "    trainer = UnsupervisedTrainer(vae, gene_dataset, train_size=0.9, use_cuda=True)\n",
    "    trainer.train(n_epochs=100, lr=0.001)\n",
    "    dic = trainer.get_all_latent_and_imputed_values()\n",
    "    dataset_posterior, latent_scvi = dic[\"all_dataset\"], dic[\"latent\"]\n",
    "    latent_scvi_starmap = latent_scvi[:gene_dataset_starmap.X.shape[0]]\n",
    "    latent_scvi_seq = latent_scvi[gene_dataset_starmap.X.shape[0]:]\n",
    "    scvi_dataset_genes = gene_dataset.gene_names\n",
    "    return latent_scvi_seq, latent_scvi_starmap, scvi_dataset_posterior, scvi_dataset_genes\n",
    "\n",
    "# Here the trainer does not have access to the information provided by gene expression levels of scRNA-seq cells\n",
    "# for genes that weren't measured in the starMAP experiment\n",
    "# We create a new Dropseq dataset with only the genes present in the starMAP experiment for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_scvi_seq, latent_scvi_starmap, scvi_dataset_posterior, scvi_dataset_genes = run_scvi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(gene_names, gene):\n",
    "    idx = 0\n",
    "    for gene_cortex in range(len(gene_names)):\n",
    "        if gene_names[gene_cortex].lower() == gene.lower():\n",
    "            idx = gene_cortex\n",
    "            print(\"Found idx \" + str(idx) + \" for gene \" + gene + \"!\")\n",
    "    return idx\n",
    "\n",
    "idx_genes_imputed = [get_index(gene_dataset_seq.gene_names, gene) for gene in genes_discarded]\n",
    "idx_scvi_genes_imputed = [get_index(scvi_dataset_genes, gene) for gene in genes_discarded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the value for a missing gene: our method vs baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_metrics(original, imputed):\n",
    "    absolute_error = np.abs(original-imputed)\n",
    "    relative_error = 0.5 * absolute_error / (1 + np.abs(original) + np.abs(imputed))\n",
    "    return {\"mean_absolute_error\": np.mean(absolute_error), \"median_absolute_error\": np.median(absolute_error), \n",
    "            \"mean_relative_error\": np.mean(relative_error), \"median_relative_error\": np.median(relative_error)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = expected_frequencies_starmap[:, idx_gene1]\n",
    "imputed /= np.sum(expected_frequencies_starmap[:, vae.indexes_to_keep], axis=1).ravel()\n",
    "imputed *= np.sum(gene_dataset_starmap.X[:, vae.indexes_to_keep], axis=1)\n",
    "plot_imputation(np.log(1+gene_dataset_starmap.X[:, idx_gene1]), np.log(1+imputed))\n",
    "print(imputation_metrics(gene_dataset_starmap.X[:, idx_gene1], imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we also use our model's latent reprensentation of the cells but rather than using the output \n",
    "#of the generative model in order to impute the missing values for the starmap cells we use a knn approach on the \n",
    "#latent space, just like the baselines\n",
    "predicted = proximity_imputation(latent_seq, gene_dataset_seq.X[:, idx_gene1], latent_starmap, k=5)\n",
    "plot_imputation(np.log(1+predicted), np.log(1+gene_dataset_starmap.X[:, idx_gene1]))\n",
    "print(imputation_metrics(gene_dataset_starmap.X[:, idx_gene1], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_gene1_scvi = get_index(gene_names_scvi, genes_discarded[0])\n",
    "predicted_scvi = proximity_imputation(latent_scvi_seq, gene_dataset_seq.X[:, idx_gene1_scvi], latent_scvi_starmap, k=5)\n",
    "plot_imputation(np.log(1+predicted_scvi), np.log(1+gene_dataset_starmap.X[:, idx_gene1_scvi]))\n",
    "print(imputation_metrics(gene_dataset_starmap.X[:, idx_gene1_scvi], predicted_scvi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_PCA = proximity_imputation(PCA_latent_seq, gene_dataset_seq.X[:, idx_gene1], PCA_latent_starmap, k=5)\n",
    "plot_imputation(np.log(1+predicted_PCA), np.log(1+gene_dataset_starmap.X[:, idx_gene1]))\n",
    "print(imputation_metrics(gene_dataset_starmap.X[:, idx_gene1], predicted_PCA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_mnn = proximity_imputation(mnn_latent_seq, gene_dataset_seq.X[:, idx_gene1], mnn_latent_starmap, k=5)\n",
    "plot_imputation(np.log(1+predicted_mnn), np.log(1+gene_dataset_starmap.X[:, idx_gene1]))\n",
    "print(imputation_metrics(gene_dataset_starmap.X[:, idx_gene1], predicted_mnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Getting a common meaningful representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def get_common_t_sne(latent_seq, latent_starmap, n_samples=1000):\n",
    "    idx_t_sne_a = np.random.permutation(len(latent_seq))[:n_samples]\n",
    "    idx_t_sne_b = np.random.permutation(len(latent_starmap))[:n_samples]\n",
    "    full_latent = np.concatenate((latent_seq[idx_t_sne_a, :], latent_starmap[idx_t_sne_b, :]))\n",
    "    if full_latent.shape[1] != 2:\n",
    "        latent = TSNE().fit_transform(full_latent)\n",
    "    if latent.shape[0] != len(idx_t_sne_a) + len(idx_t_sne_b):\n",
    "        print(\"Be careful! There might be a mistake in the downsampling of the data points\")\n",
    "    return latent[:len(idx_t_sne_a), :], latent[len(idx_t_sne_a):, :], idx_t_sne_a, idx_t_sne_b\n",
    "t_sne_seq, t_sne_starmap, idx_t_sne_seq, idx_t_sne_starmap = get_common_t_sne(latent_seq, latent_starmap, n_samples=1000)\n",
    "t_sne_PCA_seq, t_sne_PCA_starmap, idx_t_sne_PCA_seq, idx_t_sne_PCA_starmap = get_common_t_sne(PCA_latent_seq,\n",
    "                                                                                        PCA_latent_starmap,\n",
    "                                                                                        n_samples=1000)\n",
    "t_sne_mnn_seq, t_sne_mnn_starmap, idx_t_sne_mnn_seq, idx_t_sne_mnn_starmap = get_common_t_sne(mnn_latent_seq,\n",
    "                                                                                        mnn_latent_starmap,\n",
    "                                                                                        n_samples=1000)\n",
    "t_sne_scvi_seq, t_sne_scvi_starmap, idx_t_sne_scvi_seq, idx_t_sne_scvi_starmap = get_common_t_sne(latent_scvi_seq,\n",
    "                                                                                        latent_scvi_starmap,\n",
    "                                                                                        n_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our method: Embedding of the two datasets in the shared latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.concatenate((t_sne_seq, t_sne_starmap), axis=0)\n",
    "labels = np.concatenate((labels_seq[idx_t_sne_seq], labels_starmap[idx_t_sne_starmap]), axis=0)\n",
    "batch_indices = np.concatenate((np.zeros_like(labels_seq[idx_t_sne_seq]), np.ones_like(labels_starmap[idx_t_sne_starmap])), axis=0)\n",
    "trainer.train_seq.show_t_sne(None, color_by='batches and labels', latent=latent, labels =labels.ravel(), batch_indices = batch_indices.ravel(), n_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: In the OsmFISH dataset labels, there is no difference between pyramidal CA1 and pyramidal SS. That explains why the pink datapoints from Cortex's Zeisel dataset are mixed with brown datapoints from the OsmFISH dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines: Embedding of the two datasets in the shared latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.concatenate((t_sne_PCA_seq, t_sne_PCA_starmap), axis=0)\n",
    "labels = np.concatenate((labels_seq[idx_t_sne_PCA_seq], labels_starmap[idx_t_sne_PCA_starmap]), axis=0)\n",
    "batch_indices = np.concatenate((np.zeros_like(labels_seq[idx_t_sne_PCA_seq]), np.ones_like(labels_starmap[idx_t_sne_PCA_starmap])), axis=0)\n",
    "trainer.train_seq.show_t_sne(None, color_by='batches and labels', latent=latent, labels =labels.ravel(), batch_indices = batch_indices.ravel(), n_batch=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.concatenate((t_sne_mnn_seq, t_sne_mnn_starmap), axis=0)\n",
    "labels = np.concatenate((labels_seq[idx_t_sne_mnn_seq], labels_starmap[idx_t_sne_mnn_starmap]), axis=0)\n",
    "batch_indices = np.concatenate((np.zeros_like(labels_seq[idx_t_sne_mnn_seq]), np.ones_like(labels_starmap[idx_t_sne_mnn_starmap])), axis=0)\n",
    "trainer.train_seq.show_t_sne(None, color_by='batches and labels', latent=latent, labels =labels.ravel(), batch_indices = batch_indices.ravel(), n_batch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.concatenate((t_sne_scvi_seq, t_sne_scvi_starmap), axis=0)\n",
    "labels = np.concatenate((labels_seq[idx_t_sne_scvi_seq], labels_starmap[idx_t_sne_scvi_starmap]), axis=0)\n",
    "batch_indices = np.concatenate((np.zeros_like(labels_seq[idx_t_sne_scvi_seq]), np.ones_like(labels_starmap[idx_t_sne_scvi_starmap])), axis=0)\n",
    "trainer.train_seq.show_t_sne(None, color_by='batches and labels', latent=latent, labels =labels.ravel(), batch_indices = batch_indices.ravel(), n_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch entropy: How well do the datasets mix in the latent space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(entropy_batch_mixing(np.concatenate((latent_seq[idx_t_sne_seq], latent_starmap[idx_t_sne_starmap])),\n",
    "                           batches=np.concatenate((np.zeros_like(idx_t_sne_seq),\n",
    "                                                  np.ones_like(idx_t_sne_starmap)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy_batch_mixing(np.concatenate((PCA_latent_seq[idx_t_sne_PCA_seq], PCA_latent_starmap[idx_t_sne_PCA_starmap])),\n",
    "                               batches=np.concatenate((np.zeros_like(idx_t_sne_PCA_seq),\n",
    "                                                       np.ones_like(idx_t_sne_PCA_starmap)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy_batch_mixing(np.concatenate((mnn_latent_seq[idx_t_sne_mnn_seq], mnn_latent_starmap[idx_t_sne_mnn_starmap])),\n",
    "                               batches=np.concatenate((np.zeros_like(idx_t_sne_mnn_seq),\n",
    "                                                       np.ones_like(idx_t_sne_mnn_starmap)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy_batch_mixing(np.concatenate((latent_scvi_seq[idx_t_sne_scvi_seq], latent_scvi_starmap[idx_t_sne_scvi_starmap])),\n",
    "                               batches=np.concatenate((np.zeros_like(idx_t_sne_scvi_seq),\n",
    "                                                       np.ones_like(idx_t_sne_scvi_starmap)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_notebook_for_test():\n",
    "    print(\"Testing the scRNA and starMAP notebook\")\n",
    "    \n",
    "# don't mind this, it is used only when the travis build tests the notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
